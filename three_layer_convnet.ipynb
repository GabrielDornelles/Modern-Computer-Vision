{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three Layer ConvNet\n",
    "Autor: Gabriel Dornelles Monteiro, abril de 2022. Notebook nº4.\n",
    "\n",
    "Ao fim deste notebook, você deve ser capaz de entender bem o processo de otimização de neural nets em blocos, bem como os cálculos realizados. \n",
    "\n",
    "Neste notebook iremos evoluir nosso Softmax Classifier, que possuia apenas 10 neurônios. Para isso iremos adicionar uma nova camada com 1000 neurônios, e introduziremos a função de ativação não linear ReLU.\n",
    "\n",
    "Também iremos começar a trabalhar em batches para o treinamento, além de tratar o código de uma maneira mais organizada. Ao fim visualizaremos os pesos aprendidos e vamos entender quais foram as melhoras e quais ainda são nossas limitações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Boilerplate do notebook anterior\n",
    "train_dataset = CIFAR10(root=\"./\", download=True, train=True) \n",
    "val_dataset = CIFAR10(root=\"./\", download=True, train=False) \n",
    "\n",
    "train_images = np.array([np.array(train_dataset[i][0]) for i in range(len(train_dataset))])\n",
    "train_targets = np.array([np.array(train_dataset[i][1]) for i in range(len(train_dataset))])\n",
    "\n",
    "val_images = np.array([np.array(val_dataset[i][0]) for i in range(len(val_dataset))])\n",
    "val_targets = np.array([np.array(val_dataset[i][1]) for i in range(len(val_dataset))])\n",
    "\n",
    "#transforma nossas imagens 32x32x3 em vetor linha 3072\n",
    "# train_images = np.reshape(train_images, (train_images.shape[0], -1))\n",
    "# val_images = np.reshape(val_images, (val_images.shape[0], -1))\n",
    "\n",
    "# média do array train_images no eixo 0, eixo onde os índices são as imagens 3072\n",
    "mean_image = np.mean(train_images, axis = 0)\n",
    "\n",
    "# mean_image é um array de floats, para operação fazer sentido nossas imagens precisam ser também.\n",
    "train_images = train_images.astype(float)\n",
    "train_images -= mean_image\n",
    "\n",
    "val_images = val_images.astype(float)\n",
    "val_images -= mean_image\n",
    "# train_images shape: (50000, 3072)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import *\n",
    "from im2col_cython import col2im_cython, im2col_cython\n",
    "\n",
    "\n",
    "class ThreeLayerConvNet:\n",
    "    def __init__(self) -> None:\n",
    "        # Build the model here as a dictionary, weights are differentiable blocks\n",
    "        self.reg = 0.001\n",
    "        self.lr = 0.001\n",
    "\n",
    "        self.model = [\n",
    "            Conv2D(in_channels=3, num_filters=6,filter_size=5, stride=1, pad=0, std=1e-3),\n",
    "            Relu(),\n",
    "            Conv2D(in_channels=6, num_filters=16, filter_size=5, stride=1, pad=0, std=1e-3),\n",
    "            Relu(),\n",
    "            Flatten(),\n",
    "            Linear(input_size=9216, hidden_size=120, reg=self.reg, std=1e-3),\n",
    "            Relu(),\n",
    "            Linear(input_size=120, hidden_size=84, reg=self.reg, std=1e-3),\n",
    "            Relu(),\n",
    "            Linear(input_size=84, hidden_size=10, reg=self.reg, std=1e-3),\n",
    "            Softmax()\n",
    "        ]\n",
    "\n",
    "    def forward(self, x, verbose= False):\n",
    "        self.batch_size = x.shape[0]\n",
    "        #print(x.shape)\n",
    "        x = x.transpose(0,3,1,2) # Reshape to N C H W \n",
    "        for layer in self.model:\n",
    "            if verbose: print(f\"Forwarding Layer: {layer}, x shape: {x.shape}\")\n",
    "            x = layer.forward(x)\n",
    "           \n",
    "        return x\n",
    "    \n",
    "    def backward(self, y):\n",
    "        loss = self.model[-1].NLLloss(y)\n",
    "        dout = self.model[-1].backward(y)\n",
    "        for layer in reversed(self.model[:-1]):\n",
    "            dout = layer.backward(dout)\n",
    "            layer.update(lr=self.lr)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (6,400) and (150,73728) not aligned: 400 (dim 1) != 150 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-5d66e9a7fd28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mX_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoches\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-82bbfc278958>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, verbose)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Forwarding Layer: {layer}, x shape: {x.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Gabriel/Modern-Computer-Vision/layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;31m# x_cols = im2col_indices(x, w.shape[2], w.shape[3], pad, stride)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mx_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim2col_cython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cols\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (6,400) and (150,73728) not aligned: 400 (dim 1) != 150 (dim 0)"
     ]
    }
   ],
   "source": [
    "model = ThreeLayerConvNet()\n",
    "epoches = 30\n",
    "num_train = train_images.shape[0]\n",
    "batch_size = 128\n",
    "X = train_images\n",
    "y = train_targets\n",
    "for i in range(epoches):\n",
    "    batch_indices = np.random.choice(num_train, batch_size)\n",
    "    X_batch = X[batch_indices]\n",
    "    y_batch = y[batch_indices]\n",
    "    model.forward(X_batch, verbose=False)\n",
    "    loss = model.backward(y_batch)\n",
    "    if epoches%10==0:\n",
    "        print(f\"loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
