{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airi\n",
    "Autor: Gabriel Dornelles Monteiro, abril de 2022. Notebook nº3.5.\n",
    "\n",
    "Construimos dois modelos nos notebooks anteriores, e como vimos, o processo para construir e treinar os estes modelos cresceu muito do primeiro para o segundo, se torna bastante impráticavel aumentarmos os modelos com o paradigma atual que utilizamos até agora. \n",
    "\n",
    "Para isso, iremos fazer o mesmo que os modernos frameworks de Machine Learning fazem, iremos modularizar todo nosso processo, isto é, criaremos módulos que executam o que precisamos, calculam o que deve ser calculado, para que dessa maneira, nosso trabalho seja encaixar estes módulos como encaixamos peças de Lego."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De maneira geral, cada um de nossos blocos terá o seguinte formato:\n",
    "\n",
    "```py\n",
    "class AiriLayer:\n",
    "    def __init__(self):\n",
    "       pass\n",
    "  \n",
    "    def __call__(self, x):\n",
    "        self.forward(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        pass\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        pass\n",
    "    \n",
    "    def update(self, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def zero_grad(self, **kwargs):\n",
    "        pass\n",
    "```\n",
    "Onde :\n",
    "- em  ```__init__``` temos um construtor padrão para nossa classe\n",
    "- Utilizamos o magic method ```__call__``` para passar o método forward, ou seja, ao executarmos algo como:\n",
    "```py\n",
    "sample_module = AiriLayer()\n",
    "sample_module(x)\n",
    "```\n",
    "Estaremos efetivamente, executando o método forward da classe.\n",
    "- Em ```forward``` escreveremos o código que performa a computação de inferência do modelo\n",
    "- Em ```backward``` escreveremos o código que calcula os gradientes para nosso bloco, e também que ira retornar o gradiente que deve seguir seu fluxo pelo modelo\n",
    "- Em ```update``` escreveremos o código que irá realizar efetivamente todos os updates dos parâmetros pertencentes a nosso bloco.\n",
    "- Em ```zero_grad``` iremos limpar a memória zerando todos os parâmetros que mantivemos em cache durante o processo de otimização.\n",
    "\n",
    "----\n",
    "\n",
    "Vejamos o exemplo de implementação para nosso bloco Linear, que é nossa matriz de pesos:\n",
    "\n",
    "```py\n",
    "class Linear(AiriLayer):\n",
    "\n",
    "    def __init__(self, input_size=3072, hidden_size=10, reg=1e3, bias = True):\n",
    "        self.config = None\n",
    "        self.config_b = None\n",
    "        self.bias = bias\n",
    "        std = 1./ math.sqrt(input_size)\n",
    "        self.w =  np.random.randn(input_size, hidden_size) * std\n",
    "        self.b = np.zeros(hidden_size) if bias else None\n",
    "        self.reg = reg\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def forward(self, x, grad = True):\n",
    "        if grad: \n",
    "            self.x = x\n",
    "        return x@self.w + self.b\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        self.dW = self.x.T@dout\n",
    "        self.dB = dout.sum(axis=0) if self.bias else None\n",
    "        self.dW += self.reg * 2 * self.w \n",
    "        return dout@self.w.T \n",
    "        \n",
    "    def update(self, lr=1e-3):\n",
    "        self.w -= lr * self.dW\n",
    "        self.b -= lr * self.dB\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.dW = None\n",
    "        self.dB = None\n",
    "        self.x = None\n",
    "```\n",
    "\n",
    "Veja que no construtor, inicializamos a matriz normalmente, como iremos treinar modelos maiores, utilizaremos a Kaiming Initialization. Perceba que simplesmente modularizamos o processo que construimos no Softmax Classifier, de maneira a deixar isto incrementável/escalável.\n",
    "\n",
    "Para nossa layer Softmax:\n",
    "\n",
    "```py\n",
    "class Softmax(AiriLayer):\n",
    "\n",
    "    def __init__(self, loss: str = \"NLL\"):\n",
    "        self.loss_function = loss\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def forward(self, scores, grad = True):\n",
    "        self.batch_size = scores.shape[0]\n",
    "        scores -= np.max(scores, axis=1, keepdims=True)\n",
    "        scores_exp = np.exp(scores)\n",
    "        softmax_matrix = scores_exp / np.sum(scores_exp, axis=1, keepdims=True) \n",
    "        if grad:\n",
    "            self.softmax_matrix = softmax_matrix\n",
    "        return softmax_matrix\n",
    "    \n",
    "    def NLLloss(self, y):\n",
    "        loss = np.sum(-np.log(self.softmax_matrix[np.arange(self.batch_size), y]))\n",
    "        loss /= self.batch_size\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, y):\n",
    "        if self.loss_function == \"NLL\":\n",
    "            self.softmax_matrix[np.arange(self.batch_size) ,y] -= 1\n",
    "            self.softmax_matrix /= self.batch_size\n",
    "            return self.softmax_matrix\n",
    "        raise NotImplementedError(\"Unsupported Loss Function\")\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.softmax_matrix = None\n",
    "```\n",
    "\n",
    "Novamente, não há nada de novo, o código aqui presente é copiado e colado do código que trabalhamos anteriormente.\n",
    "\n",
    "\n",
    "----\n",
    "O treinamento do Softmax Classifier agora é reduzido a:\n",
    "```py\n",
    "from layers import Linear, softmax\n",
    "\n",
    "model = [\n",
    "    Linear(input_size=3072, hidden_size=10, reg=1e3, bias=True),\n",
    "    Softmax()\n",
    "    ]\n",
    "\n",
    "n_epoches = 500\n",
    "x = train_images\n",
    "y = train_targets\n",
    "\n",
    "for i in range(n_epoches):\n",
    "    \n",
    "    for layer in model:\n",
    "        x = layer.forward(x) # Não é necessario chamar o método forward. layer() é suficiente\n",
    "    \n",
    "    loss = model[-1].NLLloss(y)\n",
    "    dout = model[-1].backward(y)\n",
    "    \n",
    "    for layer in reversed(model[:-1]):\n",
    "        dout = layer.backward(dout)\n",
    "        layer.update()\n",
    "        layer.zero_grad()\n",
    "```\n",
    "\n",
    "Da mesma maneira, se quisermos agora treinar nossa Two Layer Neural Net, basta cria-la com blocos:\n",
    "\n",
    "```py\n",
    "model = [\n",
    "    Linear(input_size=3072, hidden_size=100, reg=1e3, bias=True),\n",
    "    Relu()\n",
    "    Linear(input_size=100, hidden_size=10, reg=1e3, bias=True),\n",
    "    Softmax()\n",
    "]\n",
    "```\n",
    "\n",
    "E basta treinar da mesma maneira.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convoluções\n",
    "\n",
    "TODO: Explain convs, I've done that too many times and Im not feeling in the mood to create visuals today.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
