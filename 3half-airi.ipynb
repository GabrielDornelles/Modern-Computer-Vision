{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airi\n",
    "Autor: Gabriel Dornelles Monteiro, abril de 2022. Notebook nº4.\n",
    "\n",
    "Construimos dois modelos nos notebooks anteriores, e como vimos, o processo para construir e treinar os estes modelos cresceu muito do primeiro para o segundo, se torna bastante impráticavel aumentarmos os modelos com o paradigma atual que utilizamos até agora. \n",
    "\n",
    "Para isso, iremos fazer o mesmo que os modernos frameworks de Machine Learning fazem, iremos modularizar todo nosso processo, isto é, criaremos módulos que executam o que precisamos, calculam o que deve ser calculado, para que dessa maneira, nosso trabalho seja encaixar estes módulos como encaixamos peças de Lego."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De maneira geral, cada um de nossos blocos terá o seguinte formato:\n",
    "\n",
    "```py\n",
    "class AiriLayer:\n",
    "    def __init__(self):\n",
    "       pass\n",
    "  \n",
    "    def __call__(self, x):\n",
    "        self.forward(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        pass\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        pass\n",
    "    \n",
    "    def update(self, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def zero_grad(self, **kwargs):\n",
    "        pass\n",
    "```\n",
    "Onde :\n",
    "- em  ```__init__``` temos um construtor padrão para nossa classe\n",
    "- Utilizamos o magic method ```__call__``` para passar o método forward, ou seja, ao executarmos algo como:\n",
    "```py\n",
    "sample_module = AiriLayer()\n",
    "sample_module(x)\n",
    "```\n",
    "Estaremos efetivamente, executando o método forward da classe.\n",
    "- Em ```forward``` escreveremos o código que performa a computação de inferência do modelo\n",
    "- Em ```backward``` escreveremos o código que calcula os gradientes para nosso bloco, e também que ira retornar o gradiente que deve seguir seu fluxo pelo modelo\n",
    "- Em ```update``` escreveremos o código que irá realizar efetivamente todos os updates dos parâmetros pertencentes a nosso bloco.\n",
    "- Em ```zero_grad``` Iremos limpar a memória zerando todos os parâmetros que mantivemos em cache durante o processo de otimização.\n",
    "\n",
    "----\n",
    "\n",
    "Vejamos o exemplo de implementação para nosso bloco Linear, que é nossa matriz de pesos:\n",
    "\n",
    "```py\n",
    "class Linear(AiriLayer):\n",
    "\n",
    "    def __init__(self, input_size=3072, hidden_size=10, reg=1e3, bias = True):\n",
    "        self.config = None\n",
    "        self.config_b = None\n",
    "        self.bias = bias\n",
    "        std = 1./ math.sqrt(input_size)\n",
    "        self.w =  np.random.randn(input_size, hidden_size) * std\n",
    "        self.b = np.zeros(hidden_size) if bias else None\n",
    "        self.reg = reg\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def forward(self, x, grad = True):\n",
    "        if grad: \n",
    "            self.x = x\n",
    "        return x@self.w + self.b\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        self.dW = self.x.T@dout\n",
    "        self.dB = dout.sum(axis=0) if self.bias else None\n",
    "        self.dW += self.reg * 2 * self.w \n",
    "        return dout@self.w.T \n",
    "        \n",
    "    def update(self, lr=1e-3):\n",
    "        self.w -= lr * self.dW\n",
    "        self.b -= lr * self.dB\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.dW = None\n",
    "        self.dB = None\n",
    "        self.x = None\n",
    "```\n",
    "\n",
    "Veja que no construtor, inicializamos a matriz normalmente, como iremos treinar modelos maiores, utilizaremos a Kaiming Initialization. Perceba que simplesmente modularizamos o processo que construimos no Softmax Classifier, de maneira a deixar isto incrementável/escalável.\n",
    "\n",
    "Para nossa layer Softmax:\n",
    "\n",
    "```py\n",
    "class Softmax(AiriLayer):\n",
    "\n",
    "    def __init__(self, loss: str = \"NLL\"):\n",
    "        self.loss_function = loss\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def forward(self, scores, grad = True):\n",
    "        self.batch_size = scores.shape[0]\n",
    "        scores -= np.max(scores, axis=1, keepdims=True)\n",
    "        scores_exp = np.exp(scores)\n",
    "        softmax_matrix = scores_exp / np.sum(scores_exp, axis=1, keepdims=True) \n",
    "        if grad:\n",
    "            self.softmax_matrix = softmax_matrix\n",
    "        return softmax_matrix\n",
    "    \n",
    "    def NLLloss(self, y):\n",
    "        loss = np.sum(-np.log(self.softmax_matrix[np.arange(self.batch_size), y]))\n",
    "        loss /= self.batch_size\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, y):\n",
    "        if self.loss_function == \"NLL\":\n",
    "            self.softmax_matrix[np.arange(self.batch_size) ,y] -= 1\n",
    "            self.softmax_matrix /= self.batch_size\n",
    "            return self.softmax_matrix\n",
    "        raise NotImplementedError(\"Unsupported Loss Function\")\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.softmax_matrix = None\n",
    "```\n",
    "\n",
    "Novamente, não há nada de novo, o código aqui presente é copiado e colado do código que trabalhamos anteriormente.\n",
    "\n",
    "\n",
    "----\n",
    "O treinamento do Softmax Classifier agora é reduzido a:\n",
    "```py\n",
    "from layers import Linear, softmax\n",
    "\n",
    "model = [\n",
    "    Linear(input_size=3072, hidden_size=10, reg=1e3, bias=True),\n",
    "    Softmax()\n",
    "    ]\n",
    "\n",
    "n_epoches = 500\n",
    "x = train_images\n",
    "y = train_targets\n",
    "\n",
    "for i in range(n_epoches):\n",
    "    \n",
    "    for layer in model:\n",
    "        x = layer.forward(x) # Não é necessario chamar o método forward. layer() é suficiente\n",
    "    \n",
    "    loss = model[-1].NLLloss(y)\n",
    "    dout = model[-1].backward(y)\n",
    "    \n",
    "    for layer in reversed(model[:-1]):\n",
    "        dout = layer.backward(dout)\n",
    "        layer.update()\n",
    "        layer.zero_grad()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Boilerplate do notebook anterior\n",
    "train_dataset = CIFAR10(root=\"./\", download=True, train=True) \n",
    "val_dataset = CIFAR10(root=\"./\", download=True, train=False) \n",
    "\n",
    "train_images = np.array([np.array(train_dataset[i][0]) for i in range(len(train_dataset))])\n",
    "train_targets = np.array([np.array(train_dataset[i][1]) for i in range(len(train_dataset))])\n",
    "\n",
    "val_images = np.array([np.array(val_dataset[i][0]) for i in range(len(val_dataset))])\n",
    "val_targets = np.array([np.array(val_dataset[i][1]) for i in range(len(val_dataset))])\n",
    "\n",
    "#transforma nossas imagens 32x32x3 em vetor linha 3072\n",
    "# train_images = np.reshape(train_images, (train_images.shape[0], -1))\n",
    "# val_images = np.reshape(val_images, (val_images.shape[0], -1))\n",
    "\n",
    "# média do array train_images no eixo 0, eixo onde os índices são as imagens 3072\n",
    "mean_image = np.mean(train_images, axis = 0)\n",
    "\n",
    "# mean_image é um array de floats, para operação fazer sentido nossas imagens precisam ser também.\n",
    "train_images = train_images.astype(float)\n",
    "train_images -= mean_image\n",
    "\n",
    "val_images = val_images.astype(float)\n",
    "val_images -= mean_image\n",
    "# train_images shape: (50000, 3072)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import *\n",
    "from im2col_cython import col2im_cython, im2col_cython\n",
    "\n",
    "\n",
    "class ThreeLayerConvNet:\n",
    "    def __init__(self) -> None:\n",
    "        # Build the model here as a dictionary, weights are differentiable blocks\n",
    "        self.reg = 0.001\n",
    "        self.lr = 0.001\n",
    "\n",
    "        self.model = [\n",
    "            Conv2D(in_channels=3, num_filters=6,filter_size=5, stride=1, pad=0, std=1e-3),\n",
    "            Relu(),\n",
    "            Conv2D(in_channels=6, num_filters=16, filter_size=5, stride=1, pad=0, std=1e-3),\n",
    "            Relu(),\n",
    "            Flatten(),\n",
    "            Linear(input_size=9216, hidden_size=120, reg=self.reg, std=1e-3),\n",
    "            Relu(),\n",
    "            Linear(input_size=120, hidden_size=84, reg=self.reg, std=1e-3),\n",
    "            Relu(),\n",
    "            Linear(input_size=84, hidden_size=10, reg=self.reg, std=1e-3),\n",
    "            Softmax()\n",
    "        ]\n",
    "\n",
    "    def forward(self, x, verbose= False):\n",
    "        self.batch_size = x.shape[0]\n",
    "        #print(x.shape)\n",
    "        x = x.transpose(0,3,1,2) # Reshape to N C H W \n",
    "        for layer in self.model:\n",
    "            if verbose: print(f\"Forwarding Layer: {layer}, x shape: {x.shape}\")\n",
    "            x = layer.forward(x)\n",
    "           \n",
    "        return x\n",
    "    \n",
    "    def backward(self, y):\n",
    "        loss = self.model[-1].NLLloss(y)\n",
    "        dout = self.model[-1].backward(y)\n",
    "        for layer in reversed(self.model[:-1]):\n",
    "            dout = layer.backward(dout)\n",
    "            layer.update(lr=self.lr)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ThreeLayerConvNet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1de116df9e0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mThreeLayerConvNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mepoches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnum_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ThreeLayerConvNet' is not defined"
     ]
    }
   ],
   "source": [
    "model = ThreeLayerConvNet()\n",
    "epoches = 30\n",
    "num_train = train_images.shape[0]\n",
    "batch_size = 128\n",
    "X = train_images\n",
    "y = train_targets\n",
    "for i in range(epoches):\n",
    "    batch_indices = np.random.choice(num_train, batch_size)\n",
    "    X_batch = X[batch_indices]\n",
    "    y_batch = y[batch_indices]\n",
    "    model.forward(X_batch, verbose=True)\n",
    "    loss = model.backward(y_batch)\n",
    "    if epoches%10==0:\n",
    "        print(f\"loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
